{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adoro/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:src.utils.rule_extractor:Successfully loaded spaCy model\n",
      "Device set to use cuda:0\n",
      "INFO:src.utils.rule_extractor:Successfully initialized rule scorer\n",
      "INFO:ResourceManager:ResourceManager initialized with config: src/config/resource_config.yaml\n",
      "INFO:GraphSymbolicReasoner:Successfully loaded 5 rules from data/rules.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Initializing HySym-RAG System ===\n",
      "Loading configuration...\n",
      "Initializing Resource Manager...\n",
      "Loading existing rules from data/rules.json (initially empty or minimal).\n",
      "Initializing Graph-Based Symbolic Reasoner...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:GraphSymbolicReasoner:Rule index built successfully with 5 rules.\n",
      "INFO:GraphSymbolicReasoner:Knowledge graph built with 5 nodes\n",
      "INFO:GraphSymbolicReasoner:GraphSymbolicReasoner initialized successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Neural Retriever...\n",
      "Initializing Neural Retriever with model: meta-llama/Llama-3.2-3B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.17it/s]\n",
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:QueryLogger:QueryLogger initialized successfully\n",
      "INFO:FeedbackManager:FeedbackManager initialized successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-3.2-3B loaded successfully!\n",
      "Initializing support components...\n",
      "Initializing QueryExpander...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing RuleExtractor...\n",
      "Loading evaluation dataset...\n",
      "Using DROP dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 77400/77400 [00:00<00:00, 1245770.72 examples/s]\n",
      "Generating validation split: 100%|██████████| 9535/9535 [00:00<00:00, 1372338.50 examples/s]\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Hybrid Integrator...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:FeedbackHandler:FeedbackHandler initialized successfully\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing System Control Components...\n",
      "Initializing Application...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
      "INFO:SystemControlManager:Path Selection Factors: {'complexity': 0.7636363863945007, 'resource_pressure': 0.9748793966780654}\n",
      "INFO:SystemControlManager:Selected Path: symbolic - Reason: Low query complexity favors symbolic reasoning\n",
      "INFO:SystemControlManager:Selected reasoning path: symbolic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing System with Queries ===\n",
      "\n",
      "Processing Query: Who scored the first touchdown of the game?\n",
      "Query Type: ground_truth_available\n",
      "--------------------------------------------------\n",
      "Query Complexity Score: 0.7636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.88it/s]\n",
      "INFO:GraphSymbolicReasoner:No symbolic match found.\n",
      "INFO:SystemControlManager:Path Selection Factors: {'complexity': 0.7636363863945007, 'resource_pressure': 0.9785432950659502}\n",
      "INFO:SystemControlManager:Selected Path: symbolic - Reason: Low query complexity favors symbolic reasoning\n",
      "INFO:SystemControlManager:Selected reasoning path: symbolic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Results:\n",
      "--------------------\n",
      "{'result': ['No symbolic match found.'], 'processing_time': 0.13135099411010742, 'resource_usage': {'cpu': 0.055, 'memory': 0.010000000000000009, 'gpu': 0.003663898387884723}, 'reasoning_path': 'symbolic', 'retries': 0, 'explanation': 'Reasoning Approach: symbolic | Processing Time: 0.131s | Resource Utilization: | - cpu: 5.5% | - memory: 1.0% | - gpu: 0.4%'}\n",
      "\n",
      "Resource Usage:\n",
      "CPU Delta: -4.0%\n",
      "Memory Delta: 1.0%\n",
      "GPU Delta: 0.4%\n",
      "--------------------\n",
      "Error processing query: Evaluation.evaluate() got an unexpected keyword argument 'reasoning_chain'\n",
      "\n",
      "Processing Query: How many field goals did Kris Brown kick?\n",
      "Query Type: ground_truth_available\n",
      "--------------------------------------------------\n",
      "Query Complexity Score: 0.7636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 491.14it/s]\n",
      "INFO:GraphSymbolicReasoner:No symbolic match found.\n",
      "INFO:SystemControlManager:Path Selection Factors: {'complexity': 0.625, 'resource_pressure': 0.9785432950659502}\n",
      "INFO:SystemControlManager:Selected Path: symbolic - Reason: Low query complexity favors symbolic reasoning\n",
      "INFO:SystemControlManager:Selected reasoning path: symbolic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Results:\n",
      "--------------------\n",
      "{'result': ['No symbolic match found.'], 'processing_time': 0.005845785140991211, 'resource_usage': {'cpu': 0.11800000000000001, 'memory': 0.0, 'gpu': 0.0}, 'reasoning_path': 'symbolic', 'retries': 0, 'explanation': 'Reasoning Approach: symbolic | Processing Time: 0.006s | Resource Utilization: | - cpu: 11.8% | - memory: 0.0% | - gpu: 0.0%'}\n",
      "\n",
      "Resource Usage:\n",
      "CPU Delta: -29.0%\n",
      "Memory Delta: 0.0%\n",
      "GPU Delta: 0.0%\n",
      "--------------------\n",
      "Error processing query: Evaluation.evaluate() got an unexpected keyword argument 'reasoning_chain'\n",
      "\n",
      "Processing Query: Which team won the game?\n",
      "Query Type: ground_truth_available\n",
      "--------------------------------------------------\n",
      "Query Complexity Score: 0.6250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 252.27it/s]\n",
      "INFO:GraphSymbolicReasoner:No symbolic match found.\n",
      "INFO:SystemControlManager:Path Selection Factors: {'complexity': 0.8399999499320985, 'resource_pressure': 0.9785432950659502}\n",
      "INFO:SystemControlManager:Selected Path: symbolic - Reason: High resource pressure favors symbolic path\n",
      "INFO:SystemControlManager:Selected reasoning path: symbolic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Results:\n",
      "--------------------\n",
      "{'result': ['No symbolic match found.'], 'processing_time': 0.007930517196655273, 'resource_usage': {'cpu': -0.15899999999999997, 'memory': 0.0, 'gpu': 0.0}, 'reasoning_path': 'symbolic', 'retries': 0, 'explanation': 'Reasoning Approach: symbolic | Processing Time: 0.008s | Resource Utilization: | - cpu: -15.9% | - memory: 0.0% | - gpu: 0.0%'}\n",
      "\n",
      "Resource Usage:\n",
      "CPU Delta: -26.7%\n",
      "Memory Delta: 0.0%\n",
      "GPU Delta: 0.0%\n",
      "--------------------\n",
      "Error processing query: Evaluation.evaluate() got an unexpected keyword argument 'reasoning_chain'\n",
      "\n",
      "Processing Query: How many field goals did both teams kick in the first half?\n",
      "Query Type: ground_truth_available\n",
      "--------------------------------------------------\n",
      "Query Complexity Score: 0.8400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 518.14it/s]\n",
      "INFO:GraphSymbolicReasoner:No symbolic match found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Results:\n",
      "--------------------\n",
      "{'result': ['No symbolic match found.'], 'processing_time': 0.0041310787200927734, 'resource_usage': {'cpu': 0.16699999999999998, 'memory': 0.0, 'gpu': 0.0}, 'reasoning_path': 'symbolic', 'retries': 0, 'explanation': 'Reasoning Approach: symbolic | Processing Time: 0.004s | Resource Utilization: | - cpu: 16.7% | - memory: 0.0% | - gpu: 0.0%'}\n",
      "\n",
      "Resource Usage:\n",
      "CPU Delta: -40.7%\n",
      "Memory Delta: 0.0%\n",
      "GPU Delta: 0.0%\n",
      "--------------------\n",
      "Error processing query: Evaluation.evaluate() got an unexpected keyword argument 'reasoning_chain'\n",
      "\n",
      "=== Comparison Experiment (Sample) ===\n",
      "Query                                              | Mode            | CPU Δ (%)  | Memory Δ (%)    | GPU Δ (%)  | Response\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:SystemControlManager:Computed query complexity: 0.8333\n",
      "INFO:SystemControlManager:Path Selection Factors: {'complexity': 0.8333333767950535, 'resource_pressure': 0.9785432950659502}\n",
      "INFO:SystemControlManager:Selected Path: symbolic - Reason: High resource pressure favors symbolic path\n",
      "INFO:SystemControlManager:Selected reasoning path: symbolic\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 466.92it/s]\n",
      "INFO:GraphSymbolicReasoner:No symbolic match found.\n",
      "INFO:SystemControlManager:Forced reasoning path: neural\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 228.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 527.12it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "INFO:SystemControlManager:Computed query complexity: 0.6364\n",
      "INFO:SystemControlManager:Path Selection Factors: {'complexity': 0.6363637074828148, 'resource_pressure': 0.9866038715192965}\n",
      "INFO:SystemControlManager:Selected Path: symbolic - Reason: Low query complexity favors symbolic reasoning\n",
      "INFO:SystemControlManager:Selected reasoning path: symbolic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare and contrast the film adaptations of 'Pride and Prejudice'. | Hyb.            |        0.0 |             0.0 |        0.0 | {'result': ['No symbolic match found.'], 'processing_time': 0.014783143997192383, 'resource_usage': {'cpu': 0.17600000000000002, 'memory': 0.0, 'gpu': 0.0}, 'reasoning_path': 'symbolic', 'retries': 0, 'explanation': 'Reasoning Approach: symbolic | Processing Time: 0.015s | Resource Utilization: | - cpu: 17.6% | - memory: 0.0% | - gpu: 0.0%'}\n",
      "Compare and contrast the film adaptations of 'Pride and Prejudice'. | Neural          |        0.0 |             1.2 |        0.8 | {'result': {'result': \"The 2005 film adaptation of Pride and Prejudeice, directed by Joe Wright, stands out as a visually stunning and emotionally res...\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "=== System Performance Summary ===\n",
      "\n",
      "Overall Performance:\n",
      "- Total Queries: 6\n",
      "- Average Response Time: 3.73s\n",
      "- Success Rate: 100.0%\n",
      "\n",
      "Resource Utilization:\n",
      "- CPU Usage: 0.0%\n",
      "- Memory Usage: 28.7%\n",
      "- GPU Usage: 98.7%\n",
      "\n",
      "Reasoning Path Distribution:\n",
      "- symbolic: 83.3%\n",
      "\n",
      "=== Comprehensive Academic Analysis ===\n",
      "\n",
      "Performance Analysis:\n",
      "- Average Processing Time: 2.26s\n",
      "- 95th Percentile Time: 12.28s\n",
      "\n",
      "Reasoning Analysis:\n",
      "- Average Chain Length: 0.00\n",
      "- Average Confidence: 0.00\n",
      "- Average Inference Depth: 0.00\n",
      "\n",
      "Resource Efficiency:\n",
      "- Cpu:\n",
      "  * Mean Usage: -6.0%\n",
      "  * Peak Usage: 17.6%\n",
      "  * Efficiency Score: 1.34\n",
      "- Memory:\n",
      "  * Mean Usage: 0.3%\n",
      "  * Peak Usage: 1.2%\n",
      "  * Efficiency Score: 0.73\n",
      "- Gpu:\n",
      "  * Mean Usage: 0.2%\n",
      "  * Peak Usage: 0.8%\n",
      "  * Efficiency Score: 0.81\n",
      "\n",
      "Statistical Analysis:\n",
      "Significance Tests:\n",
      "- complexity:\n",
      "  * p-value: 0.000\n",
      "  * effect size: 1.95\n",
      "- processing_time:\n",
      "  * p-value: 0.335\n",
      "  * effect size: 0.34\n",
      "\n",
      "=== Ablation Study ===\n",
      "\n",
      "Testing Configuration: No Pattern Analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 620.00it/s]\n",
      "INFO:GraphSymbolicReasoner:No symbolic match found.\n",
      "INFO:GraphSymbolicReasoner:Successfully loaded 5 rules from data/rules.json\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:GraphSymbolicReasoner:Rule index built successfully with 5 rules.\n",
      "INFO:GraphSymbolicReasoner:Knowledge graph built with 5 nodes\n",
      "INFO:GraphSymbolicReasoner:GraphSymbolicReasoner initialized successfully\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 527.65it/s]\n",
      "INFO:GraphSymbolicReasoner:No symbolic match found.\n",
      "WARNING:SystemControlManager:Resource limits exceeded, optimizing allocation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance Comparison:\n",
      "Baseline processing time: 2.05s\n",
      "Modified processing time: 2.05s\n",
      "\n",
      "Testing Configuration: Limited Hops\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "\n",
    "from src.reasoners.networkx_symbolic_reasoner import GraphSymbolicReasoner\n",
    "from src.reasoners.neural_retriever import NeuralRetriever\n",
    "from src.integrators.hybrid_integrator import HybridIntegrator\n",
    "from src.utils.rule_extractor import RuleExtractor\n",
    "from src.queries.query_logger import QueryLogger\n",
    "from src.resources.resource_manager import ResourceManager\n",
    "from src.feedback.feedback_manager import FeedbackManager\n",
    "from src.feedback.feedback_handler import FeedbackHandler\n",
    "from src.config.config_loader import ConfigLoader\n",
    "from src.queries.query_expander import QueryExpander\n",
    "from src.utils.evaluation import Evaluation\n",
    "from src.app import App\n",
    "from src.system.system_control_manager import SystemControlManager, UnifiedResponseAggregator\n",
    "from src.utils.metrics_collector import MetricsCollector\n",
    "from src.utils.device_manager import DeviceManager\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# Added for DROP dataset loading\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Suppress specific spaCy warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"spacy.util\")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "def load_hotpotqa(hotpotqa_path, max_samples=None):\n",
    "    \"\"\"\n",
    "    Loads a portion of the HotpotQA dataset.\n",
    "    Each sample includes a query, ground-truth answer,\n",
    "    combined context, and a 'type' = 'ground_truth_available'.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    with open(hotpotqa_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)  # a single large JSON array\n",
    "\n",
    "    count = 0\n",
    "    for example in data:\n",
    "        question = example['question']\n",
    "        answer = example['answer']\n",
    "        supporting_facts = example.get('supporting_facts', [])\n",
    "\n",
    "        # Flatten the context\n",
    "        context_str = []\n",
    "        for ctx_item in example.get('context', []):\n",
    "            title, sents = ctx_item[0], ctx_item[1]\n",
    "            combined_sents = \" \".join(sents)\n",
    "            context_str.append(f\"{title}: {combined_sents}\")\n",
    "        context_str = \"\\n\".join(context_str)\n",
    "\n",
    "        dataset.append({\n",
    "            \"query\": question,\n",
    "            \"answer\": answer,\n",
    "            \"context\": context_str,\n",
    "            \"type\": \"ground_truth_available\",\n",
    "            \"supporting_facts\": supporting_facts\n",
    "        })\n",
    "\n",
    "        count += 1\n",
    "        if max_samples and count >= max_samples:\n",
    "            break\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_drop(drop_dataset, max_samples=None):\n",
    "    \"\"\"\n",
    "    Loads a portion of the DROP dataset in a format similar to HotpotQA.\n",
    "    Each sample includes a query, ground-truth answer, combined context,\n",
    "    and a 'type' = 'ground_truth_available'.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    count = 0\n",
    "\n",
    "    # For example, use the 'validation' split. You can also use 'train' or 'test' if available.\n",
    "    for example in drop_dataset['validation']:\n",
    "        question = example['question']\n",
    "        passage = example['passage']\n",
    "\n",
    "        # The 'answers_spans' field typically looks like:\n",
    "        # {'spans': ['<answer string>'], 'types': ['number'] or 'span'}\n",
    "        # We'll take the first answer if it exists\n",
    "        spans = example['answers_spans']['spans']\n",
    "        answer = spans[0] if spans else \"Unknown\"\n",
    "\n",
    "        dataset.append({\n",
    "            \"query\": question,\n",
    "            \"answer\": answer,\n",
    "            \"context\": passage,\n",
    "            \"type\": \"ground_truth_available\",\n",
    "            # DROP doesn't provide \"supporting_facts\" by default,\n",
    "            # so we'll set it to None for consistency\n",
    "            \"supporting_facts\": None\n",
    "        })\n",
    "\n",
    "        count += 1\n",
    "        if max_samples and count >= max_samples:\n",
    "            break\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n=== Initializing HySym-RAG System ===\")\n",
    "\n",
    "    # 1. Load configuration\n",
    "    print(\"Loading configuration...\")\n",
    "    config = ConfigLoader.load_config(\"src/config/config.yaml\")\n",
    "    model_name = config[\"model_name\"]\n",
    "\n",
    "    # 2. Acquire a unified device from DeviceManager (optional, but recommended for consistency)\n",
    "    device = DeviceManager.get_device()\n",
    "\n",
    "    # 3. Initialize Resource Manager\n",
    "    print(\"Initializing Resource Manager...\")\n",
    "    resource_manager = ResourceManager(\n",
    "        config_path=\"src/config/resource_config.yaml\",\n",
    "        enable_performance_tracking=True,\n",
    "        history_window_size=100\n",
    "    )\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 4. (Optional) We skip extracting rules from deforestation.txt.\n",
    "    #    Instead, ensure 'data/rules.json' exists but is empty or minimal.\n",
    "    # ----------------------------------------------------------------\n",
    "    rules_path = \"data/rules.json\"\n",
    "    if not os.path.exists(rules_path):\n",
    "        with open(rules_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump([], f)\n",
    "    print(f\"Loading existing rules from {rules_path} (initially empty or minimal).\")\n",
    "\n",
    "    # 5. Initialize the Graph-Based Symbolic Reasoner\n",
    "    print(\"Initializing Graph-Based Symbolic Reasoner...\")\n",
    "    symbolic = GraphSymbolicReasoner(\n",
    "        rules_file=rules_path,\n",
    "        match_threshold=0.25,\n",
    "        max_hops=5,\n",
    "        embedding_model='all-MiniLM-L6-v2',\n",
    "        device=device  # <-- Only if your GraphSymbolicReasoner supports a 'device' param\n",
    "    )\n",
    "\n",
    "    # 6. Initialize the Neural Retriever\n",
    "    print(\"Initializing Neural Retriever...\")\n",
    "    neural = NeuralRetriever(\n",
    "        model_name,\n",
    "        use_quantization=False,\n",
    "        device=device  # <-- Only if your NeuralRetriever supports a 'device' param\n",
    "    )\n",
    "\n",
    "    # 7. Additional components\n",
    "    print(\"Initializing support components...\")\n",
    "    logger = logging.getLogger(__name__)  # Standard logger for warnings\n",
    "    query_logger = QueryLogger()\n",
    "    feedback_manager = FeedbackManager()\n",
    "    print(\"Initializing QueryExpander...\")\n",
    "    expander = QueryExpander(\n",
    "        complexity_config=\"src/config/complexity_rules.yaml\"\n",
    "    )\n",
    "    print(\"Initializing RuleExtractor...\")\n",
    "    rule_extractor = RuleExtractor()  # Instantiate RuleExtractor\n",
    "    print(\"Loading evaluation dataset...\")\n",
    "\n",
    "    # Decide which dataset to use\n",
    "    use_hotpotqa = False  # Set to True if you want to load HotpotQA\n",
    "    use_drop = True       # Set to True if you want to load DROP\n",
    "\n",
    "    test_queries = []\n",
    "    ground_truths = {}\n",
    "\n",
    "    # HotpotQA config\n",
    "    hotpotqa_path = \"data/hotpot_dev_distractor_v1.json\"\n",
    "    max_hotpot_samples = 4\n",
    "\n",
    "    # DROP config\n",
    "    max_drop_samples = 4\n",
    "\n",
    "    if use_hotpotqa and os.path.exists(hotpotqa_path):\n",
    "        print(\"Using HotpotQA dataset...\")\n",
    "        test_queries = load_hotpotqa(hotpotqa_path, max_samples=max_hotpot_samples)\n",
    "        # Build rules from HotpotQA contexts and store ground truths\n",
    "        for i, sample in enumerate(test_queries):\n",
    "            new_rules = rule_extractor.extract_hotpot_facts(sample[\"context\"], min_confidence=0.7)\n",
    "            if new_rules:\n",
    "                try:\n",
    "                    symbolic.add_dynamic_rules(new_rules)\n",
    "                except AttributeError as e:\n",
    "                    logger.warning(f\"Could not track new rules automatically (missing method?): {str(e)}\")\n",
    "            ground_truths[sample[\"query\"]] = sample[\"answer\"]\n",
    "\n",
    "    elif use_drop:\n",
    "        print(\"Using DROP dataset...\")\n",
    "        drop_data = load_dataset(\"ucinlp/drop\")\n",
    "        test_queries = load_drop(drop_data, max_samples=max_drop_samples)\n",
    "        for sample in test_queries:\n",
    "            ground_truths[sample[\"query\"]] = sample[\"answer\"]\n",
    "\n",
    "    else:\n",
    "        print(\"Warning: No valid dataset found or specified.\")\n",
    "        test_queries = []\n",
    "        ground_truths = {}\n",
    "\n",
    "    evaluator = Evaluation()\n",
    "\n",
    "    # 8. Create Hybrid Integrator\n",
    "    print(\"Creating Hybrid Integrator...\")\n",
    "    integrator = HybridIntegrator(\n",
    "        symbolic,\n",
    "        neural,\n",
    "        resource_manager,\n",
    "        expander,\n",
    "        # device=device  # <-- Only if your HybridIntegrator supports a 'device' param\n",
    "    )\n",
    "\n",
    "    # 9. System Control - Initialize MetricsCollector and pass it to SystemControlManager\n",
    "    print(\"Initializing System Control Components...\")\n",
    "    aggregator = UnifiedResponseAggregator(include_explanations=True)\n",
    "    metrics_collector = MetricsCollector()\n",
    "    system_manager = SystemControlManager(\n",
    "        hybrid_integrator=integrator,\n",
    "        resource_manager=resource_manager,\n",
    "        aggregator=aggregator,\n",
    "        metrics_collector=metrics_collector,\n",
    "        error_retry_limit=2,\n",
    "        max_query_time=10\n",
    "    )\n",
    "\n",
    "    # 10. Initialize Application\n",
    "    print(\"Initializing Application...\")\n",
    "    feedback_handler = FeedbackHandler(feedback_manager)\n",
    "    app = App(\n",
    "        symbolic=symbolic,\n",
    "        neural=neural,\n",
    "        logger=logger,\n",
    "        feedback=resource_manager,\n",
    "        evaluator=evaluator,\n",
    "        expander=expander,\n",
    "        ground_truths=ground_truths,\n",
    "        system_manager=system_manager\n",
    "    )\n",
    "\n",
    "    # 11. Possibly load a knowledge base for neural context\n",
    "    kb_path = \"data/small_knowledge_base.txt\"\n",
    "    if os.path.exists(kb_path):\n",
    "        with open(kb_path, \"r\") as kb_file:\n",
    "            context = kb_file.read()\n",
    "    else:\n",
    "        context = \"\"\n",
    "\n",
    "    print(\"\\n=== Testing System with Queries ===\")\n",
    "    for q_info in test_queries:\n",
    "        query = q_info[\"query\"]\n",
    "        the_answer = q_info.get(\"answer\", None)\n",
    "        forced_path = q_info.get(\"forced_path\", None)\n",
    "        data_type = q_info.get(\"type\", \"ground_truth_available\")\n",
    "        supporting_facts = q_info.get(\"supporting_facts\", None)  # For DROP, likely None\n",
    "\n",
    "        print(f\"\\nProcessing Query: {query}\")\n",
    "        print(f\"Query Type: {data_type}\")\n",
    "        if forced_path:\n",
    "            print(f\"Forced Path: {forced_path}\")\n",
    "        print(\"-\" * 50)\n",
    "        try:\n",
    "            initial_time = time.time()\n",
    "            complexity = expander.get_query_complexity(query)\n",
    "            print(f\"Query Complexity Score: {complexity:.4f}\")\n",
    "\n",
    "            initial_metrics = resource_manager.check_resources()\n",
    "            local_context = q_info.get(\"context\", context)\n",
    "\n",
    "            final_answer = system_manager.process_query_with_fallback(\n",
    "                query, local_context, forced_path=forced_path, query_complexity=complexity\n",
    "            )\n",
    "            final_metrics = resource_manager.check_resources()\n",
    "            resource_delta = {\n",
    "                key: final_metrics[key] - initial_metrics[key]\n",
    "                for key in final_metrics\n",
    "            }\n",
    "\n",
    "            # Collect enhanced query metrics\n",
    "            metrics_collector.collect_query_metrics(\n",
    "                query=query,\n",
    "                prediction=final_answer[0] if isinstance(final_answer, tuple) else final_answer,\n",
    "                ground_truth=the_answer,\n",
    "                reasoning_path=(\n",
    "                    symbolic.extract_reasoning_pattern(query, final_answer.get('reasoning_path', []))\n",
    "                        .get('pattern_type', 'unknown')\n",
    "                    if hasattr(symbolic, \"extract_reasoning_pattern\") else 'unknown'\n",
    "                ),\n",
    "                processing_time=time.time() - initial_time,\n",
    "                resource_usage=resource_delta,\n",
    "                complexity_score=complexity\n",
    "            )\n",
    "\n",
    "            # Track component performance (if available)\n",
    "            if isinstance(final_answer, dict):\n",
    "                metrics_collector.component_metrics['symbolic']['execution_time'].append(\n",
    "                    final_answer.get('symbolic_time', 0.0)\n",
    "                )\n",
    "                metrics_collector.component_metrics['neural']['execution_time'].append(\n",
    "                    final_answer.get('neural_time', 0.0)\n",
    "                )\n",
    "\n",
    "            query_logger.log_query(\n",
    "                query=query,\n",
    "                result=final_answer,\n",
    "                source=\"hybrid\",\n",
    "                complexity=complexity,\n",
    "                resource_usage=resource_delta\n",
    "            )\n",
    "\n",
    "            print(\"\\nProcessing Results:\")\n",
    "            print(\"-\" * 20)\n",
    "            print(final_answer)\n",
    "            print(\"\\nResource Usage:\")\n",
    "            print(f\"CPU Delta: {resource_delta['cpu'] * 100:.1f}%\")\n",
    "            print(f\"Memory Delta: {resource_delta['memory'] * 100:.1f}%\")\n",
    "            print(f\"GPU Delta: {resource_delta['gpu'] * 100:.1f}%\")\n",
    "            print(\"-\" * 20)\n",
    "\n",
    "            if data_type == \"ground_truth_available\" and the_answer is not None:\n",
    "                reasoning_chain = {}\n",
    "                if hasattr(symbolic, \"extract_reasoning_pattern\"):\n",
    "                    reasoning_chain = symbolic.extract_reasoning_pattern(\n",
    "                        query,\n",
    "                        final_answer.get('reasoning_path', [])\n",
    "                    )\n",
    "                eval_metrics = evaluator.evaluate(\n",
    "                    predictions={query: final_answer[0] if isinstance(final_answer, tuple) else final_answer},\n",
    "                    ground_truths={query: the_answer},\n",
    "                    supporting_facts={query: supporting_facts},\n",
    "                    reasoning_chain=reasoning_chain\n",
    "                )\n",
    "                print(\"\\nEvaluation Metrics:\")\n",
    "                print(f\"Similarity Score: {eval_metrics['average_semantic_similarity']:.2f}\")\n",
    "                print(f\"ROUGE-L Score: {eval_metrics['average_rougeL']:.2f}\")\n",
    "                print(f\"BLEU Score: {eval_metrics['average_bleu']:.2f}\")\n",
    "                print(f\"F1 Score: {eval_metrics['average_f1']:.2f}\")\n",
    "                if 'reasoning_analysis' in eval_metrics:\n",
    "                    print(\"\\nReasoning Analysis:\")\n",
    "                    print(f\"Pattern Type: {eval_metrics['reasoning_analysis'].get('pattern_type', 'unknown')}\")\n",
    "                    print(f\"Chain Length: {eval_metrics['reasoning_analysis'].get('chain_length', 0)}\")\n",
    "                    print(f\"Pattern Confidence: {eval_metrics['reasoning_analysis'].get('pattern_confidence', 0.0):.2f}\")\n",
    "\n",
    "        except KeyError as e:\n",
    "            print(f\"Error: Missing ground truth for query evaluation - {str(e)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {str(e)}\")\n",
    "\n",
    "    # 12. Optional Comparison Experiment\n",
    "    print(\"\\n=== Comparison Experiment (Sample) ===\")\n",
    "    comparison_queries = [\"Compare and contrast the film adaptations of 'Pride and Prejudice'.\"]\n",
    "    header = f\"{'Query':<50} | {'Mode':<15} | {'CPU Δ (%)':<10} | {'Memory Δ (%)':<15} | {'GPU Δ (%)':<10} | {'Response'}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "\n",
    "    for query in comparison_queries:\n",
    "        initial_metrics_hybrid = resource_manager.check_resources()\n",
    "        hybrid_answer = system_manager.process_query_with_fallback(query, context)\n",
    "        final_metrics_hybrid = resource_manager.check_resources()\n",
    "        hybrid_delta = {k: final_metrics_hybrid[k] - initial_metrics_hybrid[k] for k in final_metrics_hybrid}\n",
    "\n",
    "        initial_metrics_neural = resource_manager.check_resources()\n",
    "        neural_answer_raw = system_manager.process_query_with_fallback(query, context, forced_path=\"neural\")\n",
    "        neural_answer = aggregator.format_response({'result': neural_answer_raw})\n",
    "        final_metrics_neural = resource_manager.check_resources()\n",
    "        neural_delta = {k: final_metrics_neural[k] - initial_metrics_neural[k] for k in final_metrics_neural}\n",
    "\n",
    "        row_hybrid = (\n",
    "            f\"{query:<50} | {'Hyb.':<15} | {hybrid_delta['cpu'] * 100:>10.1f} \"\n",
    "            f\"| {hybrid_delta['memory'] * 100:>15.1f} | {hybrid_delta['gpu'] * 100:>10.1f} \"\n",
    "            f\"| {hybrid_answer}\"\n",
    "        )\n",
    "        row_neural = (\n",
    "            f\"{query:<50} | {'Neural':<15} | {neural_delta['cpu'] * 100:>10.1f} \"\n",
    "            f\"| {neural_delta['memory'] * 100:>15.1f} | {neural_delta['gpu'] * 100:>10.1f} \"\n",
    "            f\"| {str(neural_answer)[:150]}...\"\n",
    "        )\n",
    "        print(row_hybrid)\n",
    "        print(row_neural)\n",
    "        print(\"-\" * len(header))\n",
    "\n",
    "    print(\"\\n=== System Performance Summary ===\")\n",
    "    performance_stats = system_manager.get_performance_metrics()\n",
    "    print(\"\\nOverall Performance:\")\n",
    "    print(f\"- Total Queries: {performance_stats['total_queries']}\")\n",
    "    print(f\"- Average Response Time: {performance_stats['avg_response_time']:.2f}s\")\n",
    "    print(f\"- Success Rate: {performance_stats['success_rate']:.1f}%\")\n",
    "\n",
    "    print(\"\\nResource Utilization:\")\n",
    "    final_resources = resource_manager.check_resources()\n",
    "    print(f\"- CPU Usage: {final_resources['cpu'] * 100:.1f}%\")\n",
    "    print(f\"- Memory Usage: {final_resources['memory'] * 100:.1f}%\")\n",
    "    print(f\"- GPU Usage: {final_resources['gpu'] * 100:.1f}%\")\n",
    "\n",
    "    print(\"\\nReasoning Path Distribution:\")\n",
    "    path_stats = system_manager.get_reasoning_path_stats()\n",
    "    total_queries = performance_stats['total_queries']\n",
    "    for path, stats in path_stats.items():\n",
    "        count = stats.get('count', 0)\n",
    "        percentage = (count / total_queries) * 100 if total_queries > 0 else 0\n",
    "        print(f\"- {path}: {percentage:.1f}%\")\n",
    "\n",
    "    # Enhanced Academic Analysis Display\n",
    "    print(\"\\n=== Comprehensive Academic Analysis ===\")\n",
    "    academic_report = metrics_collector.generate_academic_report()\n",
    "\n",
    "    print(\"\\nPerformance Analysis:\")\n",
    "    print(f\"- Average Processing Time: {academic_report['performance_metrics']['processing_time']['mean']:.2f}s\")\n",
    "    print(f\"- 95th Percentile Time: {academic_report['performance_metrics']['processing_time']['percentile_95']:.2f}s\")\n",
    "\n",
    "    print(\"\\nReasoning Analysis:\")\n",
    "    if 'reasoning_analysis' in academic_report:\n",
    "        ra = academic_report['reasoning_analysis']\n",
    "        print(f\"- Average Chain Length: {ra.get('chain_characteristics', {}).get('avg_length', 0.0):.2f}\")\n",
    "        print(f\"- Average Confidence: {ra.get('chain_characteristics', {}).get('avg_confidence', 0.0):.2f}\")\n",
    "        print(f\"- Average Inference Depth: {ra.get('chain_characteristics', {}).get('avg_inference_depth', 0.0):.2f}\")\n",
    "\n",
    "    print(\"\\nResource Efficiency:\")\n",
    "    if 'efficiency_metrics' in academic_report:\n",
    "        em = academic_report['efficiency_metrics']\n",
    "        for resource, metrics in em.items():\n",
    "            if resource != 'trends':\n",
    "                print(f\"- {resource.capitalize()}:\")\n",
    "                print(f\"  * Mean Usage: {metrics.get('mean_usage', 0.0)*100:.1f}%\")\n",
    "                print(f\"  * Peak Usage: {metrics.get('peak_usage', 0.0)*100:.1f}%\")\n",
    "                print(f\"  * Efficiency Score: {metrics.get('efficiency_score', 0.0):.2f}\")\n",
    "\n",
    "    print(\"\\nStatistical Analysis:\")\n",
    "    if 'statistical_analysis' in academic_report:\n",
    "        sa = academic_report['statistical_analysis']\n",
    "        print(\"Significance Tests:\")\n",
    "        for metric, stats in sa.items():\n",
    "            if isinstance(stats, dict) and 'p_value' in stats:\n",
    "                print(f\"- {metric}:\")\n",
    "                print(f\"  * p-value: {stats['p_value']:.3f}\")\n",
    "                print(f\"  * effect size: {stats.get('effect_size', 0.0):.2f}\")\n",
    "\n",
    "    # Optional Ablation Study Section with Enhanced Reporting\n",
    "    print(\"\\n=== Ablation Study ===\")\n",
    "    ablation_results = defaultdict(dict)\n",
    "    ablation_configs = [\n",
    "        {'name': 'No Pattern Analysis', 'disable_patterns': True},\n",
    "        {'name': 'Limited Hops', 'max_hops': 2},\n",
    "        {'name': 'High Threshold', 'match_threshold': 0.5}\n",
    "    ]\n",
    "    for config in ablation_configs:\n",
    "        config_name = config['name']\n",
    "        print(f\"\\nTesting Configuration: {config_name}\")\n",
    "        try:\n",
    "            # Run baseline\n",
    "            baseline_result = system_manager.process_query_with_fallback(\n",
    "                \"What are the environmental effects of deforestation?\",\n",
    "                context\n",
    "            )\n",
    "            baseline_metrics = metrics_collector.get_real_time_metrics()\n",
    "\n",
    "            # Run modified version using a modified symbolic reasoner\n",
    "            modified_symbolic = GraphSymbolicReasoner(\n",
    "                rules_file=rules_path,\n",
    "                match_threshold=config.get('match_threshold', 0.25),\n",
    "                max_hops=config.get('max_hops', 5),\n",
    "                embedding_model='all-MiniLM-L6-v2'\n",
    "            )\n",
    "            modified_result = modified_symbolic.process_query(\n",
    "                \"What are the environmental effects of deforestation?\"\n",
    "            )\n",
    "            modified_metrics = metrics_collector.get_real_time_metrics()\n",
    "\n",
    "            ablation_results[config_name] = {\n",
    "                'baseline': baseline_metrics,\n",
    "                'modified': modified_metrics\n",
    "            }\n",
    "\n",
    "            print(\"\\nPerformance Comparison:\")\n",
    "            print(f\"Baseline processing time: {baseline_metrics['average_processing_time']:.2f}s\")\n",
    "            print(f\"Modified processing time: {modified_metrics['average_processing_time']:.2f}s\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in ablation study for {config_name}: {str(e)}\")\n",
    "\n",
    "    print(\"\\n=== System Performance Summary (Extended) ===\")\n",
    "    pattern_metrics = symbolic.get_reasoning_metrics()\n",
    "    print(f\"- Average Chain Length: {pattern_metrics['path_analysis']['average_length']:.2f}\")\n",
    "    print(f\"- Average Confidence: {pattern_metrics['confidence_analysis']['mean_confidence']:.2f}\")\n",
    "    print(\"\\nPattern Distribution:\")\n",
    "    for length, frequency in pattern_metrics['path_analysis']['path_distribution'].items():\n",
    "        print(f\"- {length}-hop paths: {frequency * 100:.1f}%\")\n",
    "\n",
    "    print(\"\\n=== End of Run ===\")\n",
    "    print(\"\\n=== Academic Evaluation Results ===\")\n",
    "    print(json.dumps(academic_report, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /home/adoro/miniconda3/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/adoro/miniconda3/lib/python3.12/site-packages (from datasets) (2.1.2)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /home/adoro/miniconda3/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/adoro/miniconda3/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/adoro/miniconda3/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /home/adoro/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.12-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/adoro/miniconda3/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: packaging in /home/adoro/miniconda3/lib/python3.12/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/adoro/miniconda3/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/adoro/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.18.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/adoro/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/adoro/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/adoro/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/adoro/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/adoro/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/adoro/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/adoro/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/adoro/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/adoro/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading aiohttp-3.11.12-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading pyarrow-19.0.1-cp312-cp312-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (283 kB)\n",
      "Downloading multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
      "Downloading propcache-0.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (243 kB)\n",
      "Downloading yarl-1.18.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (336 kB)\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, frozenlist, dill, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohappyeyeballs-2.4.6 aiohttp-3.11.12 aiosignal-1.3.2 datasets-3.3.2 dill-0.3.8 frozenlist-1.5.0 multidict-6.1.0 multiprocess-0.70.16 propcache-0.3.0 pyarrow-19.0.1 xxhash-3.5.0 yarl-1.18.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
